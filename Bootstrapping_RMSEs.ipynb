{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d98eb21",
   "metadata": {
    "id": "0d98eb21"
   },
   "source": [
    "# Setup\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6158c26e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:23:35.505892Z",
     "start_time": "2022-09-11T07:23:34.040143Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1661339299204,
     "user": {
      "displayName": "Amichai Baichman-Ka",
      "userId": "16914562961023845270"
     },
     "user_tz": -180
    },
    "id": "6158c26e",
    "outputId": "5235f36c-a2ce-4153-df9a-b41e0b031d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statannot import add_stat_annotation\n",
    "\n",
    "import warnings\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4ac1c",
   "metadata": {
    "id": "5ac4ac1c"
   },
   "source": [
    "# K2: singles and pair effect experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1f66c",
   "metadata": {
    "id": "c7a1f66c"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185e9ba",
   "metadata": {
    "id": "6185e9ba"
   },
   "source": [
    "### Create lists of sample types for later use  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7989b84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:23:35.521737Z",
     "start_time": "2022-09-11T07:23:35.505892Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1661339303922,
     "user": {
      "displayName": "Amichai Baichman-Ka",
      "userId": "16914562961023845270"
     },
     "user_tz": -180
    },
    "id": "7989b84a"
   },
   "outputs": [],
   "source": [
    "isolates=['BI', 'CB', 'CF', 'EA', 'EC1', 'EC10', 'EC11', 'EC12','EC14', \n",
    "          'EC16','EC17', 'EC18', 'EC9', 'Ecoli', 'Gh11','Gh12','Gh16',\n",
    "          'Gh22', 'Gh24','Gh28', 'Gh29', 'Gh30', 'Gh35', 'Gh36','Gh38',\n",
    "          'Gh40', 'Gh43', 'Gh44', 'Gh50', 'Gh54', 'Gh59', 'Gh60','Gh61',\n",
    "          'Gh62', 'Gh67', 'Gh68', 'Gh9','KA', 'LA', 'OGI19-L','OGI33-L',\n",
    "          'Og122', 'Og84', 'Og85', 'Og87', 'Og90','Og92', 'PAg1','PAg3', \n",
    "          'PAl', 'PH', 'PK', 'PP', 'ParPR2', 'RP1', 'RP1_A','RP1_B', \n",
    "          'RP2', 'SF1', 'SF2', 'Ut12', 'Ut14', 'Ut22']\n",
    "monos=  ['Mono1','Mono2','Mono']\n",
    "blanks= ['Blank1','Blank2']\n",
    "\n",
    "timepoints=['t0','t1','t2','t3']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87322667",
   "metadata": {
    "id": "87322667"
   },
   "source": [
    "### Load and orginze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a3a69a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:23:36.077753Z",
     "start_time": "2022-09-11T07:23:35.522734Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "error",
     "timestamp": 1661339454322,
     "user": {
      "displayName": "Amichai Baichman-Ka",
      "userId": "16914562961023845270"
     },
     "user_tz": -180
    },
    "id": "e4a3a69a",
    "outputId": "76c728f1-48c4-42c9-cf69-3af88239ded7"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Pair chips with their data files\n",
    "data_files=dict(zip(range(1,13),[\"EC_A\",\"EC_B\",\"EA_A\",\"EA_B\",\"RP1_A\",\"RP1_B\",\"BI_A\",\"BI_B\",\n",
    "                                 \"CF_A\",\"CF_B\",\"PAg_A\",\"PAg_B\"]))\n",
    "\n",
    "#Load data from all chips into dictionary\n",
    "chips_dict={}\n",
    "for chip in range(1,13):\n",
    "    chips_dict[data_files[chip]]=pd.read_csv('Data/kChip_data/k2/chip'+str(chip)+'.csv')\n",
    "    \n",
    "    area_mode=chips_dict[data_files[chip]]['t0_Area'].mode()[0]\n",
    "    margin=area_mode*0.3\n",
    "    chips_dict[data_files[chip]]=chips_dict[data_files[chip]][(chips_dict[data_files[chip]].t0_Area>(area_mode-margin)) &\n",
    "                           (chips_dict[data_files[chip]].t0_Area<(area_mode+margin)) & (chips_dict[data_files[chip]].Total==2)]\n",
    "    chips_dict[data_files[chip]]['chip']=chr(ord('@')+(chip%2+1))\n",
    "    \n",
    "#Merge replicate chips\n",
    "chips_dict2={}\n",
    "for focal in [\"EC\",\"EA\",\"RP1\",\"BI\",\"CF\",\"PAg\"]:\n",
    "    chips_dict2[focal]=chips_dict[focal+'_A'].append(chips_dict[focal+'_B'])\n",
    "\n",
    "chips_dict=chips_dict2\n",
    "\n",
    "\"\"\"Subtract t0 value for each well individually (so we are measuring added biomass)\"\"\"\n",
    "\n",
    "n_chips_dict={}\n",
    "for chip in chips_dict:\n",
    "    df=chips_dict[chip].copy()\n",
    "\n",
    "    #normalize each well to its own starting value\n",
    "    df.t1=df.t1-df.t0\n",
    "    df.t2=df.t2-df.t0\n",
    "    df.t3=df.t3-df.t0\n",
    "    df.t0=df.t0-df.t0\n",
    "    \n",
    "    #ensure no negative values or zero to avoid inf values later on\n",
    "    df[timepoints]=df[timepoints].clip(lower=1)\n",
    "    \n",
    "    n_chips_dict[chip]=df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00da585",
   "metadata": {
    "id": "d00da585"
   },
   "source": [
    "### Load growth curve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe1c42c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:23:44.005772Z",
     "start_time": "2022-09-11T07:23:36.077753Z"
    },
    "id": "8fe1c42c"
   },
   "outputs": [],
   "source": [
    "#dictionary to match samples to identifiers\n",
    "phylo=pd.read_excel('Data/Strains.xlsx')\n",
    "r_phylo_dict=dict(zip(phylo['Identifier'],phylo['Sample Name']))\n",
    "\n",
    "\n",
    "gc_data=(pd.read_csv('Data/Isolate_profiling/gc_data.csv')\n",
    "         .replace(r_phylo_dict)\n",
    "         .replace({'EColi':'Ecoli'})\n",
    "         .drop_duplicates(subset='sample',)\n",
    "         .set_index('sample'))\n",
    "\n",
    "max_od_dict=dict(zip(gc_data.index,gc_data.max_od))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6a46af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:23:44.021393Z",
     "start_time": "2022-09-11T07:23:44.005772Z"
    },
    "id": "5e6a46af"
   },
   "outputs": [],
   "source": [
    "def modified_S (row):\n",
    "    if row[\"mono_s1_effect\"]>0 and row[\"mono_s2_effect\"]>0:\n",
    "        row['modified_strongest']=row[[\"mono_s1_effect\",\"mono_s2_effect\"]].max()\n",
    "    else: row['modified_strongest']=row[[\"mono_s1_effect\",\"mono_s2_effect\"]].min()\n",
    "    return row\n",
    "\n",
    "def comm_size(row):\n",
    "    \n",
    "    #assign community size\n",
    "    if row['sample1']==row['sample2']: \n",
    "        row['comm_size']='single'\n",
    "    elif row['sample1']=='Mono': \n",
    "        row['comm_size']='single'    \n",
    "    elif row['sample2']=='Mono': \n",
    "        row['comm_size']='single'\n",
    "    else: row['comm_size']='pair'    \n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dac28a",
   "metadata": {},
   "source": [
    "## Test on original dataset and compare values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a7ebd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:24:01.378592Z",
     "start_time": "2022-09-11T07:23:44.021393Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "orngl_effects_dict={}\n",
    "\n",
    "for chip in list(n_chips_dict.keys()):\n",
    "    tp='t1'\n",
    "    data=n_chips_dict[chip].copy()\n",
    "    focal_mono_df=data[(data['sample1'].isin(monos))&\n",
    "                           (data['sample2'].isin(monos))][tp]\n",
    "    focal_mono_median=focal_mono_df.median()\n",
    "    \n",
    "    data[['sample1','sample2']]= data[['sample1','sample2']].replace({'RP1_A':'RP1','RP1_B':'RP1',\n",
    "                                                                      'Blank1':np.nan,'Blank2':np.nan,\n",
    "                                                                      'Mono1':np.nan,'Mono2':np.nan,\n",
    "                                                                      'Mono':np.nan})\n",
    "\n",
    "    data.sample1 = data.sample1.fillna(data.sample2)\n",
    "    data.sample2 = data.sample2.fillna(data.sample1)\n",
    "    \n",
    "    #get data for pair\n",
    "    effect_data=data.groupby(['sample1', 'sample2'],\n",
    "                             as_index=False)[tp].median()\n",
    "    \n",
    "    effect_data['combined_effect']=log(effect_data[tp]/focal_mono_median)\n",
    "    \n",
    "    effect_data['Count']=data.groupby(['sample1', 'sample2'],\n",
    "                                      as_index=False)[tp].count()[tp]\n",
    "\n",
    "#     add data for each bug on its own\n",
    "    mono_t=(effect_data[(effect_data.sample1==effect_data.sample2)|\n",
    "                        (effect_data.sample1.isin(monos))|\n",
    "                        (effect_data.sample2.isin(monos))]\n",
    "          [['sample1','sample2','combined_effect', 'Count']])\n",
    "\n",
    "    mono_t.sample1=mono_t.sample1.replace(['Mono'], np.nan)\n",
    "    mono_t.sample1 = mono_t.sample1.fillna(mono_t.sample2)\n",
    "\n",
    "    mono_t.sample2=mono_t.sample2.replace(['Mono'], np.nan)\n",
    "    mono_t.sample2 = mono_t.sample2.fillna(mono_t.sample1)\n",
    "\n",
    "    mono=pd.DataFrame(mono_t.groupby(['sample1','sample2'],\n",
    "                                    as_index=False)['combined_effect'].mean())\n",
    "    mono['Count']=mono_t.groupby(['sample1','sample2'],\n",
    "                                as_index=False)['Count'].sum()['Count']           \n",
    "          \n",
    "    mono=mono.rename(columns={'combined_effect':'effect'})\n",
    "    \n",
    "    mono['max_od'] = mono.sample1.map(max_od_dict)\n",
    "    \n",
    "    monos1=mono.add_prefix('mono_s1_')\n",
    "    effect_data=effect_data.merge(monos1, left_on='sample1',\n",
    "                                  right_on='mono_s1_sample1').drop('mono_s1_sample1',axis=1)\n",
    "    monos2=mono.add_prefix('mono_s2_')\n",
    "    effect_data=effect_data.merge(monos2, left_on='sample2',\n",
    "                                  right_on='mono_s2_sample1').drop('mono_s2_sample1',axis=1)\n",
    "\n",
    "    effect_data['strongest'] = effect_data.apply(lambda x: max(x[[\"mono_s1_effect\",\"mono_s2_effect\"]],\n",
    "                                                               key=abs), axis=1)\n",
    "    \n",
    "    effect_data['mean'] = effect_data[[\"mono_s1_effect\",\"mono_s2_effect\"]].mean(axis=1)\n",
    "    \n",
    "    effect_data['additive']=(effect_data[\"mono_s1_effect\"]+\n",
    "                             effect_data[\"mono_s2_effect\"])\n",
    "    \n",
    "    effect_data['OD_W_mean']=(((effect_data[\"mono_s1_effect\"]*effect_data[\"mono_s1_max_od\"])+\n",
    "                                  (effect_data[\"mono_s2_effect\"]*effect_data[\"mono_s2_max_od\"]))/\n",
    "                                  (effect_data[\"mono_s1_max_od\"]+effect_data[\"mono_s2_max_od\"]))  \n",
    "    \n",
    "    effect_data['focal']=chip\n",
    "    effect_data=effect_data.apply(modified_S, axis=1)\n",
    "    effect_data=effect_data.apply(comm_size, axis=1)\n",
    "        \n",
    "    effect_data=effect_data[effect_data['Count']>2]\n",
    "    orngl_effects_dict[chip]=effect_data\n",
    "    \n",
    "#merge into single dataframe\n",
    "original_dataset=orngl_effects_dict['EC'].copy()\n",
    "for chip in list(orngl_effects_dict.keys())[1:]:\n",
    "    original_dataset=original_dataset.append(orngl_effects_dict[chip])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e500c37a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:24:01.409839Z",
     "start_time": "2022-09-11T07:24:01.378592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additive 0.663\n",
      "mean 0.464\n",
      "OD_W_mean 0.361\n",
      "strongest 0.263\n",
      "modified_strongest 0.244\n"
     ]
    }
   ],
   "source": [
    "pairs=original_dataset[original_dataset.comm_size=='pair']#.drop_duplicates()\n",
    "\n",
    "for m in ['additive','mean', 'OD_W_mean', 'strongest','modified_strongest']:\n",
    "    \n",
    "    effects_df2=pairs[[m,'combined_effect']].dropna()\n",
    "    iqr=sp.stats.iqr(effects_df2.combined_effect)\n",
    "\n",
    "    print(m,(((mean_squared_error(effects_df2[m],effects_df2['combined_effect']))**0.5)/iqr).round(3))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d88861",
   "metadata": {
    "id": "35d88861"
   },
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712108ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:24:35.743364Z",
     "start_time": "2022-09-11T07:24:01.412628Z"
    },
    "id": "712108ca",
    "outputId": "a1260cec-f503-4122-90dc-66f224898956"
   },
   "outputs": [],
   "source": [
    "repeats=1001\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "k2_effects_dict={}\n",
    "df_counter=1\n",
    "for chip in list(n_chips_dict.keys()):\n",
    "    tp='t1'\n",
    "    data2=n_chips_dict[chip].copy()\n",
    "    for i in range(1,repeats):\n",
    "        data=data2.sample(len(data2), replace=True, random_state=i, ignore_index=True)\n",
    "\n",
    "        focal_mono_df=data[(data['sample1'].isin(monos))&\n",
    "                       (data['sample2'].isin(monos))][tp]\n",
    "        focal_mono_median=focal_mono_df.median()\n",
    "\n",
    "        data[['sample1','sample2']]= data[['sample1','sample2']].replace({'RP1_A':'RP1','RP1_B':'RP1',\n",
    "                                                                          'Blank1':np.nan,'Blank2':np.nan,\n",
    "                                                                          'Mono1':np.nan,'Mono2':np.nan,\n",
    "                                                                          'Mono':np.nan})\n",
    "\n",
    "        data.sample1 = data.sample1.fillna(data.sample2)\n",
    "        data.sample2 = data.sample2.fillna(data.sample1)\n",
    "\n",
    "        #get data for pair\n",
    "        effect_data=data.groupby(['sample1', 'sample2'],\n",
    "                                 as_index=False)[tp].median()\n",
    "\n",
    "        effect_data['combined_effect']=log(effect_data[tp]/focal_mono_median)\n",
    "\n",
    "        effect_data['Count']=data.groupby(['sample1', 'sample2'],\n",
    "                                          as_index=False)[tp].count()[tp]\n",
    "\n",
    "    #     add data for each bug on its own\n",
    "        mono_t=(effect_data[(effect_data.sample1==effect_data.sample2)|\n",
    "                            (effect_data.sample1.isin(monos))|\n",
    "                            (effect_data.sample2.isin(monos))]\n",
    "              [['sample1','sample2','combined_effect', 'Count']])\n",
    "\n",
    "        mono_t.sample1=mono_t.sample1.replace(['Mono'], np.nan)\n",
    "        mono_t.sample1 = mono_t.sample1.fillna(mono_t.sample2)\n",
    "\n",
    "        mono_t.sample2=mono_t.sample2.replace(['Mono'], np.nan)\n",
    "        mono_t.sample2 = mono_t.sample2.fillna(mono_t.sample1)\n",
    "\n",
    "        mono=pd.DataFrame(mono_t.groupby(['sample1','sample2'],\n",
    "                                        as_index=False)['combined_effect'].mean())\n",
    "        mono['Count']=mono_t.groupby(['sample1','sample2'],\n",
    "                                    as_index=False)['Count'].sum()['Count']           \n",
    "\n",
    "        mono=mono.rename(columns={'combined_effect':'effect'})\n",
    "\n",
    "        mono['max_od'] = mono.sample1.map(max_od_dict)\n",
    "\n",
    "        monos1=mono.add_prefix('mono_s1_')\n",
    "        effect_data=effect_data.merge(monos1, left_on='sample1',\n",
    "                                      right_on='mono_s1_sample1').drop('mono_s1_sample1',axis=1)\n",
    "        monos2=mono.add_prefix('mono_s2_')\n",
    "        effect_data=effect_data.merge(monos2, left_on='sample2',\n",
    "                                      right_on='mono_s2_sample1').drop('mono_s2_sample1',axis=1)\n",
    "\n",
    "        effect_data['strongest'] = effect_data.apply(lambda x: max(x[[\"mono_s1_effect\",\"mono_s2_effect\"]],\n",
    "                                                                   key=abs), axis=1)\n",
    "\n",
    "        effect_data['mean'] = effect_data[[\"mono_s1_effect\",\"mono_s2_effect\"]].mean(axis=1)\n",
    "\n",
    "        effect_data['additive']=(effect_data[\"mono_s1_effect\"]+effect_data[\"mono_s2_effect\"])\n",
    "\n",
    "        effect_data['OD_W_mean']=(((effect_data[\"mono_s1_effect\"]*effect_data[\"mono_s1_max_od\"])+\n",
    "                                   (effect_data[\"mono_s2_effect\"]*effect_data[\"mono_s2_max_od\"]))/\n",
    "                                   (effect_data[\"mono_s1_max_od\"]+effect_data[\"mono_s2_max_od\"]))   \n",
    "        effect_data['focal']=chip\n",
    "\n",
    "        effect_data=effect_data.apply(modified_S, axis=1)\n",
    "        effect_data=effect_data.apply(comm_size, axis=1)\n",
    "        effect_data=effect_data[effect_data['Count']>2]\n",
    "        \n",
    "        k2_effects_dict[df_counter]=effect_data\n",
    "        \n",
    "      \n",
    "        df_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ec8588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:24:35.806650Z",
     "start_time": "2022-09-11T07:24:35.743364Z"
    },
    "id": "63ec8588",
    "outputId": "28ad4f1f-037c-45aa-d829-4b5ddc030abf"
   },
   "outputs": [],
   "source": [
    "merged_effects_dict={}\n",
    "\n",
    "for start_chip in range(1,repeats):\n",
    "    temp_effect_df=pd.DataFrame()\n",
    "        \n",
    "    for chip in range(start_chip,len(k2_effects_dict)+1,repeats-1):\n",
    "        temp_effect_df=temp_effect_df.append(k2_effects_dict[chip])\n",
    "\n",
    "    merged_effects_dict[start_chip]=temp_effect_df.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed6026",
   "metadata": {
    "id": "e3ed6026"
   },
   "source": [
    "## Calculate RMSE for each bootstrapped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "443c2948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:24:35.837875Z",
     "start_time": "2022-09-11T07:24:35.807666Z"
    },
    "id": "443c2948"
   },
   "outputs": [],
   "source": [
    "k2_summ_df=pd.DataFrame(columns=['additive','mean', 'OD_W_mean', 'strongest','modified_strongest'])\n",
    "counter=1\n",
    "for effects_df in merged_effects_dict.values():\n",
    "    effects_df=effects_df[effects_df.comm_size=='pair']\n",
    "    \n",
    "    for m in ['additive','mean', 'OD_W_mean', 'strongest','modified_strongest']:\n",
    "        \n",
    "        effects_df2=effects_df[[m,'combined_effect']].dropna()\n",
    "        iqr=sp.stats.iqr(effects_df2.combined_effect)\n",
    "\n",
    "        k2_summ_df.at[counter, m]=(((mean_squared_error(effects_df2[m],effects_df2['combined_effect']))**0.5)/iqr)\n",
    "        \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709a4a66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:24:35.869528Z",
     "start_time": "2022-09-11T07:24:35.838872Z"
    },
    "id": "709a4a66",
    "outputId": "1c9a2d4b-50e5-40bb-f369-d81ed88cf5f5",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>CoV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>additive</th>\n",
       "      <td>0.663</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.478</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OD_W_mean</th>\n",
       "      <td>0.383</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strongest</th>\n",
       "      <td>0.292</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modified_strongest</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean  median    std    CoV\n",
       "additive            0.663   0.663  0.026  0.039\n",
       "mean                0.478   0.478  0.010  0.021\n",
       "OD_W_mean           0.383   0.383  0.008  0.021\n",
       "strongest           0.292   0.292  0.005  0.017\n",
       "modified_strongest  0.280   0.280  0.004  0.015"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"Mean\\n\",k2_summ_df.mean().round(3))\n",
    "# print(\"Median\\n\",k2_summ_df.median().round(3))\n",
    "# print(\"STD\\n\", k2_summ_df.std().round(3))\n",
    "# print(\"Coefficient of variation\\n\",(k2_summ_df.std()/k2_summ_df.mean()).round(3))\n",
    "\n",
    "k2_sumsum=pd.DataFrame()\n",
    "# k2_sumsum['original']=(.mean().round(3))\n",
    "k2_sumsum['mean']=(k2_summ_df.mean().round(3))\n",
    "k2_sumsum['mean']=(k2_summ_df.mean().round(3))\n",
    "k2_sumsum['median']=(k2_summ_df.median().round(3))\n",
    "k2_sumsum['std']=(k2_summ_df.std().round(3))\n",
    "k2_sumsum['CoV']=((k2_summ_df.std()/k2_summ_df.mean()).round(3))\n",
    "k2_sumsum\n",
    "\n",
    "# additive 0.663\n",
    "# mean 0.464\n",
    "# OD_W_mean 0.37\n",
    "# strongest 0.263\n",
    "# modified_strongest 0.244\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b41f52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T05:45:00.973706Z",
     "start_time": "2022-08-30T05:45:00.967684Z"
    }
   },
   "source": [
    "## Models by focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5009be93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:24:36.011800Z",
     "start_time": "2022-09-11T07:24:35.870525Z"
    }
   },
   "outputs": [],
   "source": [
    "k2_summ_focal_df=pd.DataFrame()\n",
    "counter=0\n",
    "for effects_df in merged_effects_dict.values():\n",
    "    for f in effects_df.focal.unique():\n",
    "        effects_df2=effects_df[effects_df.focal==f]\n",
    "        for v in ['additive','mean', 'OD_W_mean', 'strongest','modified_strongest']:\n",
    "            effects_df3=effects_df2[[v,'combined_effect']].dropna()\n",
    "            iqr=sp.stats.iqr(effects_df3.combined_effect)\n",
    "\n",
    "            k2_summ_focal_df.at[counter, f+'_'+v]=(((mean_squared_error(effects_df3[v],\n",
    "                                                                        effects_df3['combined_effect']))**0.5)/iqr)\n",
    "\n",
    "    counter+=1    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc04cb0",
   "metadata": {},
   "source": [
    "## Models by sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0681594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:25:37.433001Z",
     "start_time": "2022-09-11T07:25:22.629344Z"
    }
   },
   "outputs": [],
   "source": [
    "def qual_mono (row):\n",
    "    row['qual_monos']=','.join(sorted(list(row[['qual_e_1','qual_e_2']])))\n",
    "    return (row)\n",
    "\n",
    "qm_dict={'-,-':'Negative-Negative','+,-':'Postive-Negative','+,+':'Postive-Postive'}\n",
    "\n",
    "k2_summ_qm_df=pd.DataFrame()\n",
    "counter=0\n",
    "for effects_df in merged_effects_dict.values():\n",
    "    effects_df['qual_e_1'] = np.where(effects_df['mono_s1_effect'] > 0, '+', '-')\n",
    "    effects_df['qual_e_2'] = np.where(effects_df['mono_s2_effect'] > 0, '+', '-')\n",
    "    effects_df=effects_df.apply(qual_mono, axis=1)\n",
    "    \n",
    "    for qm in effects_df.qual_monos.unique():\n",
    "        effects_df2=effects_df[effects_df.qual_monos==qm]\n",
    "\n",
    "        for v in ['additive','mean', 'OD_W_mean', 'strongest','modified_strongest']:\n",
    "            effects_df3=effects_df2[[v,'combined_effect']].dropna()\n",
    "            iqr=sp.stats.iqr(effects_df3.combined_effect)\n",
    "\n",
    "            k2_summ_qm_df.at[counter, qm_dict[qm]+'__'+v]=(((mean_squared_error(effects_df3[v],\n",
    "                                                                     effects_df3['combined_effect']))**0.5)/iqr)\n",
    "\n",
    "    counter+=1    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee92dcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:24:36.326412Z",
     "start_time": "2022-09-11T07:23:34.067Z"
    }
   },
   "outputs": [],
   "source": [
    "# k2_summ_df.to_csv('Data/Bootstrap_data/k2_summ_df.csv')\n",
    "# k2_summ_qm_df.to_csv('Data/Bootstrap_data/k2_summ_qm_df.csv')\n",
    "# k2_summ_focal_df.to_csv('Data/Bootstrap_data/k2_summ_focal_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b269d",
   "metadata": {
    "id": "5e4b269d"
   },
   "source": [
    "# Trios with E coli Focal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04656b",
   "metadata": {
    "id": "4d04656b"
   },
   "source": [
    "## Setup\n",
    "### Load and normalize data from all chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f371f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:26:08.820857Z",
     "start_time": "2022-09-11T07:26:08.382930Z"
    },
    "id": "02f371f6"
   },
   "outputs": [],
   "source": [
    "#redefine timepoints without t3 for this experiment\n",
    "timepoints=['t0','t1','t2']\n",
    "\n",
    "\n",
    "#Import data\n",
    "path=\"Data/kChip_data/k3/\"\n",
    "c1=pd.read_csv(path+'chip1.csv')[[\"t0\", \"t1\",\"t2\",\"sample1\",\"sample2\",\"sample3\"]].dropna()\n",
    "c2=pd.read_csv(path+'chip2.csv')[[\"t0\", \"t1\",\"t2\",\"sample1\",\"sample2\",\"sample3\"]].dropna()\n",
    "c3=pd.read_csv(path+'chip3.csv')[[\"t0\", \"t1\",\"t2\",\"sample1\",\"sample2\",\"sample3\"]].dropna()\n",
    "\n",
    "#label each chip seperately and merge to one dataset\n",
    "c1['chip']='A'\n",
    "c2['chip']='B'\n",
    "c3['chip']='C'\n",
    "k3=c1.append(c2).append(c3)\n",
    "\n",
    "#normalize to t0\n",
    "k3[timepoints]=k3[timepoints].sub(k3['t0'], axis=0)\n",
    "k3[timepoints]=k3[timepoints].clip(lower=1).round(3)\n",
    "k3=k3.replace(r_phylo_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9664c487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:26:08.836815Z",
     "start_time": "2022-09-11T07:26:08.822852Z"
    },
    "id": "9664c487",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pair_effects(row):\n",
    "    \n",
    "    try:\n",
    "        row['pair_12_effect']=pair_effect_dict[\"['\"+row['sample1']+\"', '\"\n",
    "                                               +row['sample2']+\"']\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            row['pair_12_effect']=pair_effect_dict[\"['\"+row['sample2']+\"', '\"\n",
    "                                               +row['sample1']+\"']\"]\n",
    "        except KeyError: row['pair_12_effect']=np.nan\n",
    "            \n",
    "    try:\n",
    "        row['pair_13_effect']=pair_effect_dict[\"['\"+row['sample1']+\"', '\"\n",
    "                                               +row['sample3']+\"']\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            row['pair_13_effect']=pair_effect_dict[\"['\"+row['sample3']+\"', '\"\n",
    "                                               +row['sample1']+\"']\"]\n",
    "        except KeyError: row['pair_13_effect']=np.nan\n",
    "            \n",
    "    try:\n",
    "        row['pair_23_effect']=pair_effect_dict[\"['\"+row['sample3']+\"', '\"\n",
    "                                               +row['sample2']+\"']\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            row['pair_23_effect']=pair_effect_dict[\"['\"+row['sample2']+\"', '\"\n",
    "                                               +row['sample3']+\"']\"]\n",
    "        except KeyError: row['pair_23_effect']=np.nan\n",
    "            \n",
    "    row['additive_p']= row[\"pair_12_effect\"]+row[\"pair_13_effect\"]+row[\"pair_23_effect\"]\n",
    "    row['mean_p']= mean(row[['pair_12_effect', 'pair_13_effect','pair_23_effect']])\n",
    "    row['strongest_p']=max(row[['pair_12_effect', 'pair_13_effect','pair_23_effect']], key=abs)\n",
    "                           \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6665eba",
   "metadata": {
    "id": "a6665eba"
   },
   "source": [
    "## Orginzing and calculating data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46052b2a",
   "metadata": {
    "id": "46052b2a"
   },
   "source": [
    "### Calculating effect by trio- orginal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55fe1164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:26:10.883480Z",
     "start_time": "2022-09-11T07:26:08.838809Z"
    }
   },
   "outputs": [],
   "source": [
    "#Agregate mono growth from mono and mono+blank wells\n",
    "mono_df=pd.DataFrame(columns=list(k3))\n",
    "for i in list(itertools.product(['Mono1','Blank1'],repeat=3))[:-1]:\n",
    "    data=k3[(k3['sample1']==i[0])&(k3['sample2']==i[1])&(k3['sample3']==i[2])]\n",
    "    mono_df=mono_df.append(data)\n",
    "    \n",
    "focal_mono_median=mono_df.t1.median()\n",
    "\n",
    "k3_wB=k3.copy()\n",
    "\n",
    "k3_wB.sample1=k3_wB.sample1.replace(['Blank1'], np.nan)\n",
    "k3_wB.sample2=k3_wB.sample2.replace(['Blank1'], np.nan)\n",
    "k3_wB.sample3=k3_wB.sample3.replace(['Blank1'], np.nan)\n",
    "\n",
    "k3_wB.sample1 = k3_wB.sample1.fillna(k3_wB.sample2)\n",
    "k3_wB.sample1 = k3_wB.sample1.fillna(k3_wB.sample3)\n",
    "\n",
    "k3_wB.sample2 = k3_wB.sample2.fillna(k3_wB.sample1)\n",
    "k3_wB.sample2 = k3_wB.sample2.fillna(k3_wB.sample3)\n",
    "\n",
    "k3_wB.sample3 = k3_wB.sample3.fillna(k3_wB.sample1)\n",
    "k3_wB.sample3 = k3_wB.sample3.fillna(k3_wB.sample2)\n",
    "\n",
    "k3_wB[['sample1','sample2','sample3']]=k3_wB[['sample1','sample2',\n",
    "                                                          'sample3']].fillna('Blank1')\n",
    "\n",
    "k3_wB_bs=k3_wB.sample(len(k3), replace=False)\n",
    "\n",
    "k3_effect_data=k3_wB_bs.groupby(['sample1','sample2','sample3'], \n",
    "                         as_index=False)['t1'].median()\n",
    "\n",
    "k3_effect_data['combined_effect']=log(k3_effect_data['t1']/focal_mono_median)\n",
    "\n",
    "k3_effect_data['Count']=k3_wB_bs.groupby(['sample1', 'sample2','sample3'], \n",
    "                                  as_index=False)['t1'].count()['t1']\n",
    "\n",
    "#add data for each bug on its own\n",
    "mono=(k3_effect_data[(k3_effect_data.sample1==k3_effect_data.sample2)&\n",
    "                  (k3_effect_data.sample1==k3_effect_data.sample3)][['sample1','combined_effect', \n",
    "                                                               'Count']]\n",
    "       .rename(columns={'combined_effect':'effect'}))\n",
    "mono['qual_e'] = np.where(mono['effect'] > 0, '+', '-')\n",
    "mono['max_od'] = mono.sample1.map(max_od_dict)\n",
    "\n",
    "\n",
    "monos1=mono.add_prefix('mono_s1_')\n",
    "k3_effect_data=k3_effect_data.merge(monos1, how='left', left_on='sample1',\n",
    "                              right_on='mono_s1_sample1').drop('mono_s1_sample1',\n",
    "                                                               axis=1)\n",
    "\n",
    "monos2=mono.add_prefix('mono_s2_')\n",
    "k3_effect_data=k3_effect_data.merge(monos2, how='left', left_on='sample2',\n",
    "                              right_on='mono_s2_sample1').drop('mono_s2_sample1',\n",
    "                                                               axis=1)\n",
    "\n",
    "monos3=mono.add_prefix('mono_s3_')\n",
    "k3_effect_data=k3_effect_data.merge(monos3, how='left', left_on='sample3',\n",
    "                              right_on='mono_s3_sample1').drop('mono_s3_sample1',\n",
    "                                                               axis=1)\n",
    "\n",
    "\n",
    "k3_effect_data['additive_s']=(k3_effect_data[\"mono_s1_effect\"]+\n",
    "                            k3_effect_data[\"mono_s2_effect\"]+\n",
    "                            k3_effect_data[\"mono_s3_effect\"])\n",
    "\n",
    "k3_effect_data['mean_s']=mean(k3_effect_data[[\"mono_s1_effect\",\"mono_s2_effect\",\"mono_s3_effect\"]],axis=1)\n",
    "\n",
    "k3_effect_data['strongest_s'] = k3_effect_data.apply(lambda x: max(x[[\"mono_s1_effect\",\"mono_s2_effect\",\n",
    "                                                                    \"mono_s3_effect\"]],key=abs), axis=1)\n",
    "\n",
    "k3_effect_data['OD_W_mean']=(((k3_effect_data[\"mono_s1_effect\"]*k3_effect_data[\"mono_s1_max_od\"])+\n",
    "                              (k3_effect_data[\"mono_s2_effect\"]*k3_effect_data[\"mono_s2_max_od\"])+\n",
    "                              (k3_effect_data[\"mono_s3_effect\"]*k3_effect_data[\"mono_s3_max_od\"]))/\n",
    "                              sum(k3_effect_data[[\"mono_s1_max_od\",\"mono_s2_max_od\",\"mono_s3_max_od\"]],axis=1))\n",
    "\n",
    "\n",
    "original_k3=k3_effect_data[k3_effect_data['Count']>2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd6b83ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:26:28.728464Z",
     "start_time": "2022-09-11T07:26:10.884468Z"
    }
   },
   "outputs": [],
   "source": [
    "for r in original_k3.index:\n",
    "    row=original_k3.loc[r]\n",
    "\n",
    "    if ((row.sample1!=row.sample2)&\n",
    "        (row.sample1!=row.sample3)&\n",
    "        (row.sample2!=row.sample3)&\n",
    "        (row.sample1!='Mono1')&(row.sample2!='Mono1')&(row.sample3!='Mono1')):\n",
    "\n",
    "        original_k3.at[r,'comm_size']='trio'\n",
    "\n",
    "    elif(((row.sample1==row.sample2)&(row.sample1!=row.sample3)&\n",
    "          (row.sample1!='Mono1')&(row.sample3!='Mono1'))|\n",
    "\n",
    "         ((row.sample1==row.sample3)&(row.sample1!=row.sample2)&\n",
    "          (row.sample1!='Mono1')&(row.sample2!='Mono1'))|\n",
    "\n",
    "         ((row.sample2==row.sample3)&(row.sample2!=row.sample1)&\n",
    "          (row.sample1!='Mono1')&(row.sample3!='Mono1'))|\n",
    "\n",
    "         ((row.sample1=='Mono1')&(row.sample2!='Mono1')&\n",
    "          (row.sample3!='Mono1')&(row.sample2!=row.sample3))|\n",
    "         ((row.sample2=='Mono1')&(row.sample1!='Mono1')&\n",
    "          (row.sample3!='Mono1')&(row.sample1!=row.sample3))|\n",
    "         ((row.sample3=='Mono1')&(row.sample2!='Mono1')&\n",
    "          (row.sample1!='Mono1')&(row.sample2!=row.sample1))):\n",
    "\n",
    "        original_k3.at[r,'comm_size']='pair'\n",
    "\n",
    "    else: original_k3.at[r,'comm_size']='single'\n",
    "\n",
    "#calculate pair effects\n",
    "pair_data=original_k3[original_k3.comm_size=='pair']\n",
    "\n",
    "#merge wells with monos with wells without \n",
    "pair_data[['sample1','sample2','sample3',]]=pair_data[['sample1','sample2','sample3',]].replace(['Mono1'], np.nan)\n",
    "\n",
    "pair_data.sample1 = pair_data.sample1.fillna(pair_data.sample2)\n",
    "pair_data.sample1 = pair_data.sample1.fillna(pair_data.sample3)\n",
    "\n",
    "pair_data.sample2 = pair_data.sample2.fillna(pair_data.sample1)\n",
    "pair_data.sample2 = pair_data.sample2.fillna(pair_data.sample3)\n",
    "\n",
    "pair_data.sample3 = pair_data.sample3.fillna(pair_data.sample1)\n",
    "pair_data.sample3 = pair_data.sample3.fillna(pair_data.sample2)\n",
    "\n",
    "#add column for with pair for each row\n",
    "pair_data['trio'] =((pair_data[['sample1','sample2','sample3']]).values.tolist())\n",
    "pair_data['pair'] = pair_data.apply(lambda x: sorted(list(set(x['trio']))),axis=1)\n",
    "pair_data['pair'] = pair_data.apply(lambda x: (str(x['pair'])),axis=1)\n",
    "pair_data['trio'] = pair_data.apply(lambda x: (str(x['trio'])),axis=1)\n",
    "\n",
    "#organize data for comparison between diff starting densities\n",
    "melted=pair_data.melt(id_vars=['pair'], value_vars=['combined_effect'],)\n",
    "pair_pairs=pd.DataFrame(columns=['pair','ratio1','ratio2','ratio3'],index=range(len(list(melted.pair.unique()))))\n",
    "\n",
    "#new dataset with 2:1 and 1:2 ratios in each row\n",
    "for n,ii in enumerate(list(melted.pair.unique())):\n",
    "    if len(melted[melted.pair==ii])==3:\n",
    "        pair_pairs.loc[n]['pair']=ii\n",
    "        pair_pairs.loc[n]['ratio1']=(list((melted[melted.pair==ii]).value))[0]\n",
    "        pair_pairs.loc[n]['ratio2']=(list((melted[melted.pair==ii]).value))[1]\n",
    "        pair_pairs.loc[n]['ratio3']=(list((melted[melted.pair==ii]).value))[2]\n",
    "    elif len(melted[melted.pair==i])==2:\n",
    "        pair_pairs.loc[n]['pair']=ii\n",
    "        pair_pairs.loc[n]['ratio1']=(list((melted[melted.pair==ii]).value))[0]\n",
    "        pair_pairs.loc[n]['ratio2']=(list((melted[melted.pair==ii]).value))[1]\n",
    "\n",
    "    #16 pairs only have one ratio, but are included here to use in the dictionary downstream \n",
    "    elif len(melted[melted.pair==ii])==1:\n",
    "        pair_pairs.loc[n]['pair']=i\n",
    "        pair_pairs.loc[n]['ratio1']=(list((melted[melted.pair==ii]).value))[0]\n",
    "        pair_pairs.loc[n]['ratio2']=(list((melted[melted.pair==ii]).value))[0]\n",
    "\n",
    "    else: continue\n",
    "\n",
    "pair_effect_dict=melted.groupby('pair').mean()['value'].to_dict()\n",
    "\n",
    "#calculate trio predictoins based on pairs\n",
    "original_k3=original_k3.apply(pair_effects,axis=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4704121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:26:28.774877Z",
     "start_time": "2022-09-11T07:26:28.729465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      "additive_s     2.855\n",
      "mean_s         1.318\n",
      "OD_W_mean      1.036\n",
      "strongest_s    0.508\n",
      "additive_p     4.794\n",
      "mean_p         0.616\n",
      "strongest_p    0.418\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "k3_summ_original=pd.DataFrame(columns=['additive_s','mean_s','OD_W_mean','strongest_s',\n",
    "                                 'additive_p','mean_p','strongest_p'])\n",
    "counter=0\n",
    "for v in ['additive_s','mean_s','OD_W_mean','strongest_s',\n",
    "          'additive_p','mean_p','strongest_p']:\n",
    "    effects_df=original_k3[original_k3.comm_size=='trio']\n",
    "    effects_df2=effects_df[[v,'combined_effect']].dropna()\n",
    "    iqr=sp.stats.iqr(effects_df2.combined_effect)\n",
    "\n",
    "    k3_summ_original.at[counter, v]=(((mean_squared_error(effects_df2[v],effects_df2['combined_effect']))**0.5)/iqr)\n",
    "        \n",
    "counter+=1 \n",
    "\n",
    "print(\"Mean\")\n",
    "print(k3_summ_original.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4b21a",
   "metadata": {},
   "source": [
    "### Calculating effect by trio- Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a5d572",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:26:32.632290Z",
     "start_time": "2022-09-11T07:26:28.776877Z"
    },
    "id": "a0a5d572"
   },
   "outputs": [],
   "source": [
    "repeats=1001\n",
    "k3_effects_dict={}\n",
    "\n",
    "        \n",
    "#Agregate mono growth from mono and mono+blank wells\n",
    "mono_df=pd.DataFrame(columns=list(k3))\n",
    "for i in list(itertools.product(['Mono1','Blank1'],repeat=3))[:-1]:\n",
    "    data=k3[(k3['sample1']==i[0])&(k3['sample2']==i[1])&(k3['sample3']==i[2])]\n",
    "    mono_df=mono_df.append(data)\n",
    "    \n",
    "focal_mono_median=mono_df.t1.median()\n",
    "\n",
    "k3_wB=k3.copy()\n",
    "\n",
    "k3_wB.sample1=k3_wB.sample1.replace(['Blank1'], np.nan)\n",
    "k3_wB.sample2=k3_wB.sample2.replace(['Blank1'], np.nan)\n",
    "k3_wB.sample3=k3_wB.sample3.replace(['Blank1'], np.nan)\n",
    "\n",
    "k3_wB.sample1 = k3_wB.sample1.fillna(k3_wB.sample2)\n",
    "k3_wB.sample1 = k3_wB.sample1.fillna(k3_wB.sample3)\n",
    "\n",
    "k3_wB.sample2 = k3_wB.sample2.fillna(k3_wB.sample1)\n",
    "k3_wB.sample2 = k3_wB.sample2.fillna(k3_wB.sample3)\n",
    "\n",
    "k3_wB.sample3 = k3_wB.sample3.fillna(k3_wB.sample1)\n",
    "k3_wB.sample3 = k3_wB.sample3.fillna(k3_wB.sample2)\n",
    "\n",
    "k3_wB[['sample1','sample2','sample3']]=k3_wB[['sample1','sample2',\n",
    "                                                          'sample3']].fillna('Blank1')\n",
    "\n",
    "for i in range(1,repeats):\n",
    "\n",
    "    k3_wB_bs=k3_wB.sample(len(k3), replace=True, random_state=i)\n",
    "\n",
    "    k3_effect_data=k3_wB_bs.groupby(['sample1','sample2','sample3'], \n",
    "                             as_index=False)['t1'].median()\n",
    "\n",
    "    k3_effect_data['combined_effect']=log(k3_effect_data['t1']/focal_mono_median)\n",
    "\n",
    "    k3_effect_data['Count']=k3_wB_bs.groupby(['sample1', 'sample2','sample3'], \n",
    "                                      as_index=False)['t1'].count()['t1']\n",
    "\n",
    "    #add data for each bug on its own\n",
    "    mono=(k3_effect_data[(k3_effect_data.sample1==k3_effect_data.sample2)&\n",
    "                      (k3_effect_data.sample1==k3_effect_data.sample3)][['sample1','combined_effect', \n",
    "                                                                   'Count']]\n",
    "           .rename(columns={'combined_effect':'effect'}))\n",
    "    mono['qual_e'] = np.where(mono['effect'] > 0, '+', '-')\n",
    "    mono['max_od'] = mono.sample1.map(max_od_dict)\n",
    "\n",
    "\n",
    "    monos1=mono.add_prefix('mono_s1_')\n",
    "    k3_effect_data=k3_effect_data.merge(monos1, how='left', left_on='sample1',\n",
    "                                  right_on='mono_s1_sample1').drop('mono_s1_sample1',\n",
    "                                                                   axis=1)\n",
    "\n",
    "    monos2=mono.add_prefix('mono_s2_')\n",
    "    k3_effect_data=k3_effect_data.merge(monos2, how='left', left_on='sample2',\n",
    "                                  right_on='mono_s2_sample1').drop('mono_s2_sample1',\n",
    "                                                                   axis=1)\n",
    "\n",
    "    monos3=mono.add_prefix('mono_s3_')\n",
    "    k3_effect_data=k3_effect_data.merge(monos3, how='left', left_on='sample3',\n",
    "                                  right_on='mono_s3_sample1').drop('mono_s3_sample1',\n",
    "                                                                   axis=1)\n",
    "\n",
    "\n",
    "    k3_effect_data['additive_s']=(k3_effect_data[\"mono_s1_effect\"]+\n",
    "                                k3_effect_data[\"mono_s2_effect\"]+\n",
    "                                k3_effect_data[\"mono_s3_effect\"])\n",
    "\n",
    "    k3_effect_data['mean_s']=mean(k3_effect_data[[\"mono_s1_effect\",\"mono_s2_effect\",\"mono_s3_effect\"]],axis=1)\n",
    "\n",
    "    k3_effect_data['strongest_s'] = k3_effect_data.apply(lambda x: max(x[[\"mono_s1_effect\",\"mono_s2_effect\",\n",
    "                                                                        \"mono_s3_effect\"]],key=abs), axis=1)\n",
    "    \n",
    "    k3_effect_data['OD_W_mean']=(((k3_effect_data[\"mono_s1_effect\"]*k3_effect_data[\"mono_s1_max_od\"])+\n",
    "                                  (k3_effect_data[\"mono_s2_effect\"]*k3_effect_data[\"mono_s2_max_od\"])+\n",
    "                                  (k3_effect_data[\"mono_s3_effect\"]*k3_effect_data[\"mono_s3_max_od\"]))/\n",
    "                                  sum(k3_effect_data[[\"mono_s1_max_od\",\"mono_s2_max_od\",\"mono_s3_max_od\"]],axis=1))\n",
    "\n",
    "\n",
    "    k3_effects_dict[i]=k3_effect_data[k3_effect_data['Count']>2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb380091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:27:04.415252Z",
     "start_time": "2022-09-11T07:26:32.634288Z"
    },
    "id": "eb380091"
   },
   "outputs": [],
   "source": [
    "for i in range(1,repeats):\n",
    "    #call each bootstrap individually\n",
    "    temp_data=k3_effects_dict[i]\n",
    "    \n",
    "    #add comm_size column\n",
    "    for r in temp_data.index:\n",
    "        row=temp_data.loc[r]\n",
    "\n",
    "        if ((row.sample1!=row.sample2)&\n",
    "            (row.sample1!=row.sample3)&\n",
    "            (row.sample2!=row.sample3)&\n",
    "            (row.sample1!='Mono1')&(row.sample2!='Mono1')&(row.sample3!='Mono1')):\n",
    "\n",
    "            temp_data.at[r,'comm_size']='trio'\n",
    "\n",
    "        elif(((row.sample1==row.sample2)&(row.sample1!=row.sample3)&\n",
    "              (row.sample1!='Mono1')&(row.sample3!='Mono1'))|\n",
    "\n",
    "             ((row.sample1==row.sample3)&(row.sample1!=row.sample2)&\n",
    "              (row.sample1!='Mono1')&(row.sample2!='Mono1'))|\n",
    "\n",
    "             ((row.sample2==row.sample3)&(row.sample2!=row.sample1)&\n",
    "              (row.sample1!='Mono1')&(row.sample3!='Mono1'))|\n",
    "\n",
    "             ((row.sample1=='Mono1')&(row.sample2!='Mono1')&\n",
    "              (row.sample3!='Mono1')&(row.sample2!=row.sample3))|\n",
    "             ((row.sample2=='Mono1')&(row.sample1!='Mono1')&\n",
    "              (row.sample3!='Mono1')&(row.sample1!=row.sample3))|\n",
    "             ((row.sample3=='Mono1')&(row.sample2!='Mono1')&\n",
    "              (row.sample1!='Mono1')&(row.sample2!=row.sample1))):\n",
    "                \n",
    "            temp_data.at[r,'comm_size']='pair'\n",
    "\n",
    "        else: temp_data.at[r,'comm_size']='single'\n",
    "\n",
    "    #calculate pair effects\n",
    "    pair_data=temp_data[temp_data.comm_size=='pair']\n",
    "    \n",
    "    #merge wells with monos with wells without \n",
    "    pair_data[['sample1','sample2','sample3',]]=pair_data[['sample1','sample2','sample3',]].replace(['Mono1'], np.nan)\n",
    "\n",
    "    pair_data.sample1 = pair_data.sample1.fillna(pair_data.sample2)\n",
    "    pair_data.sample1 = pair_data.sample1.fillna(pair_data.sample3)\n",
    "\n",
    "    pair_data.sample2 = pair_data.sample2.fillna(pair_data.sample1)\n",
    "    pair_data.sample2 = pair_data.sample2.fillna(pair_data.sample3)\n",
    "\n",
    "    pair_data.sample3 = pair_data.sample3.fillna(pair_data.sample1)\n",
    "    pair_data.sample3 = pair_data.sample3.fillna(pair_data.sample2)\n",
    "\n",
    "    #add column for with pair for each row\n",
    "    pair_data['trio'] =((pair_data[['sample1','sample2','sample3']]).values.tolist())\n",
    "    pair_data['pair'] = pair_data.apply(lambda x: sorted(list(set(x['trio']))),axis=1)\n",
    "    pair_data['pair'] = pair_data.apply(lambda x: (str(x['pair'])),axis=1)\n",
    "    pair_data['trio'] = pair_data.apply(lambda x: (str(x['trio'])),axis=1)\n",
    "\n",
    "    #organize data for comparison between diff starting densities\n",
    "    melted=pair_data.melt(id_vars=['pair'], value_vars=['combined_effect'],)\n",
    "    pair_pairs=pd.DataFrame(columns=['pair','ratio1','ratio2','ratio3'],index=range(len(list(melted.pair.unique()))))\n",
    "\n",
    "    #new dataset with 2:1 and 1:2 ratios in each row\n",
    "    for n,ii in enumerate(list(melted.pair.unique())):\n",
    "        if len(melted[melted.pair==ii])==3:\n",
    "            pair_pairs.loc[n]['pair']=ii\n",
    "            pair_pairs.loc[n]['ratio1']=(list((melted[melted.pair==ii]).value))[0]\n",
    "            pair_pairs.loc[n]['ratio2']=(list((melted[melted.pair==ii]).value))[1]\n",
    "            pair_pairs.loc[n]['ratio3']=(list((melted[melted.pair==ii]).value))[2]\n",
    "        elif len(melted[melted.pair==i])==2:\n",
    "            pair_pairs.loc[n]['pair']=ii\n",
    "            pair_pairs.loc[n]['ratio1']=(list((melted[melted.pair==ii]).value))[0]\n",
    "            pair_pairs.loc[n]['ratio2']=(list((melted[melted.pair==ii]).value))[1]\n",
    "    \n",
    "        #16 pairs only have one ratio, but are included here to use in the dictionary downstream \n",
    "        elif len(melted[melted.pair==ii])==1:\n",
    "            pair_pairs.loc[n]['pair']=i\n",
    "            pair_pairs.loc[n]['ratio1']=(list((melted[melted.pair==ii]).value))[0]\n",
    "            pair_pairs.loc[n]['ratio2']=(list((melted[melted.pair==ii]).value))[0]\n",
    "\n",
    "        else: continue\n",
    "\n",
    "    pair_effect_dict=melted.groupby('pair').mean()['value'].to_dict()\n",
    "    \n",
    "    #calculate trio predictoins based on pairs\n",
    "    k3_effects_dict[i]=temp_data.apply(pair_effects,axis=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b9b78c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:27:04.494186Z",
     "start_time": "2022-09-11T07:27:04.417254Z"
    },
    "id": "9b9b78c7",
    "outputId": "d388cb47-09d8-4538-c24a-07e604285da4"
   },
   "outputs": [],
   "source": [
    "k3_summ_df=pd.DataFrame(columns=['additive_s','mean_s','OD_W_mean','strongest_s',\n",
    "                                 'additive_p','mean_p','strongest_p'])\n",
    "counter=0\n",
    "for effects_df in k3_effects_dict.values():\n",
    "    for v in ['additive_s','mean_s','OD_W_mean','strongest_s',\n",
    "              'additive_p','mean_p','strongest_p']:\n",
    "        effects_df=effects_df[effects_df.comm_size=='trio']\n",
    "        effects_df2=effects_df[[v,'combined_effect']].dropna()\n",
    "        iqr=sp.stats.iqr(effects_df2.combined_effect)\n",
    "        \n",
    "        k3_summ_df.at[counter, v]=(((mean_squared_error(effects_df2[v],effects_df2['combined_effect']))**0.5)/iqr)\n",
    "        \n",
    "    counter+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d9014e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:27:04.542084Z",
     "start_time": "2022-09-11T07:27:04.500170Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>CoV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>additive_s</th>\n",
       "      <td>2.855</td>\n",
       "      <td>2.714</td>\n",
       "      <td>2.714</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_s</th>\n",
       "      <td>1.318</td>\n",
       "      <td>1.222</td>\n",
       "      <td>1.222</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OD_W_mean</th>\n",
       "      <td>1.036</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strongest_s</th>\n",
       "      <td>0.508</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>additive_p</th>\n",
       "      <td>4.794</td>\n",
       "      <td>4.252</td>\n",
       "      <td>4.252</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_p</th>\n",
       "      <td>0.616</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strongest_p</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             original   mean  median    std    CoV\n",
       "additive_s      2.855  2.714   2.714  0.172  0.063\n",
       "mean_s          1.318  1.222   1.222  0.055  0.045\n",
       "OD_W_mean       1.036  0.977   0.977  0.034  0.035\n",
       "strongest_s     0.508  0.793   0.793  0.065  0.082\n",
       "additive_p      4.794  4.252   4.252  0.070  0.016\n",
       "mean_p          0.616  0.674   0.674  0.038  0.056\n",
       "strongest_p     0.418  0.577   0.577  0.026  0.044"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k3_sumsum=pd.DataFrame()\n",
    "k3_sumsum['original']=(k3_summ_original.mean().round(3))\n",
    "k3_sumsum['mean']=(k3_summ_df.mean().round(3))\n",
    "k3_sumsum['mean']=(k3_summ_df.mean().round(3))\n",
    "k3_sumsum['median']=(k3_summ_df.median().round(3))\n",
    "k3_sumsum['std']=(k3_summ_df.std().round(3))\n",
    "k3_sumsum['CoV']=((k3_summ_df.std()/k3_summ_df.mean()).round(3))\n",
    "k3_sumsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cec69dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-11T07:27:04.557344Z",
     "start_time": "2022-09-11T07:27:04.543004Z"
    }
   },
   "outputs": [],
   "source": [
    "# k3_summ_df.to_csv('Data/Bootstrap_data/k3_summ_df.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bootstrapping_RMSEs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1215px",
    "left": "262px",
    "top": "111.125px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
